{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systmes with Explicit Ratings\n",
    "\n",
    "This notebook demos various methods of building and analyzing recommender systems that deal with explicit rating data.  We will explore baseline algorithms, neighborhood methods, matrix factorization-based ( SVD, SVD++, NMF). \n",
    "\n",
    "The implementation of the model is from [Surprise](https://github.com/NicolasHug/Surprise), which stands for `Simple Python RecommendatIon System Engine.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "MovieLens data sets were collected by the GroupLens Research Project\n",
    "at the University of Minnesota.\n",
    "\n",
    "This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies.\n",
    "* Each user has rated at least 20 movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 ratings (1-5) from 943 users and 1682 movies \n",
      "user 166 has rated the least movies with 20 times\n",
      "user 405 has rated the most movies with 737 times\n",
      "movie 1348 was rated the least with 1 times\n",
      "movie 50 was rated the most with 583 times\n"
     ]
    }
   ],
   "source": [
    "from surprise import Dataset\n",
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "# Load the movielens-100k dataset (download it if needed),\n",
    "data = Dataset.load_builtin('ml-100k')\n",
    "\n",
    "ucounter = defaultdict(int); icounter = defaultdict(int)\n",
    "for user,item,rating,ts in data.raw_ratings:\n",
    "    ucounter[user] += 1\n",
    "    icounter[item] += 1\n",
    "print (\"{} ratings (1-5) from {} users and {} movies \".format(len(data.raw_ratings), len(ucounter), len(icounter)) )\n",
    "\n",
    "maxuser = max(ucounter, key=lambda u: ucounter[u])\n",
    "minuser = min(ucounter, key=lambda u: ucounter[u])\n",
    "print (\"user {} has rated the least movies with {} times\".format(minuser, ucounter[minuser]))\n",
    "print (\"user {} has rated the most movies with {} times\".format(maxuser, ucounter[maxuser]))\n",
    "\n",
    "maxmovie = max(icounter, key=lambda i: icounter[i])\n",
    "minmovie = min(icounter, key=lambda i: icounter[i])\n",
    "\n",
    "print (\"movie {} was rated the least with {} times\".format(minmovie, icounter[minmovie]))\n",
    "print (\"movie {} was rated the most with {} times\".format(maxmovie, icounter[maxmovie]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample random trainset and testset\n",
    "\n",
    "Test set is made of 25% of the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 1636 of items, 943 of users, and 75000 of ratings in the trainig set \n",
      "there are 25000 of records in the test set \n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainset, testset = train_test_split(data, test_size=.25)\n",
    "print (\"there are {} of items, {} of users, and {} of ratings in the trainig set \".format(trainset.n_items, trainset.n_users, trainset.n_ratings))\n",
    "print (\"there are {} of records in the test set \".format(len(testset))) #test set is a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('214', '137', 4.0),\n",
       " ('440', '312', 5.0),\n",
       " ('671', '222', 1.0),\n",
       " ('173', '294', 5.0),\n",
       " ('7', '227', 3.0),\n",
       " ('21', '100', 5.0),\n",
       " ('288', '318', 4.0),\n",
       " ('490', '987', 3.0),\n",
       " ('449', '462', 5.0),\n",
       " ('293', '223', 4.0)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility function: \n",
    "Define the prediction function that we will use to spot check the model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Utility function to print out the prediction results from Surprise\n",
    "def printPrediction(pred):\n",
    "    s = 'user: {uid:<10} '.format(uid=pred.uid)\n",
    "    s += 'item: {iid:<10} '.format(iid=pred.iid)\n",
    "    if pred.r_ui is not None:\n",
    "        s += 'r_ui = {r_ui:1.2f}   '.format(r_ui=pred.r_ui)\n",
    "    else:\n",
    "        s += 'r_ui = None   '\n",
    "    s += 'est = {est:1.2f}   '.format(est=pred.est)\n",
    "    print (s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testPrediction(model):\n",
    "    for uid, iid, rating in testset[16:25]:\n",
    "        printPrediction(model.predict(uid, iid, rating, verbose=False))\n",
    "    uid = str(196)  # raw user id (as in the ratings file). They are **strings**!\n",
    "    iid = str(302)  # raw item id (as in the ratings file). They are **strings**!\n",
    "    # get a prediction for specific users and items.\n",
    "    pred = model.predict(uid, iid, r_ui=4, verbose=False)\n",
    "    printPrediction(pred)\n",
    "    #return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Prediction\n",
    "\n",
    "Algorithm predicting a random rating based on the distribution of the training set, which is assumed to be normal.\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is generated from a normal distribution $N(\\mu, \\sigma^2)$ where $\\mu$ and $\\sigma$ are estimated from the training data using Maximum Likelihood Estimation:\n",
    "\n",
    "$$ \\hat{\\mu} = \\frac{1}{|R_{train}|} \\sum_{r_{ui} \\in R_{train}}r_{ui} $$\n",
    "\n",
    "$$\\hat{\\sigma} = \\sqrt{\\sum_{r_{ui} \\in R_{train}}\n",
    "        \\frac{(r_{ui} - \\hat{\\mu})^2}{|R_{train}|}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.5295\n",
      "user: 312        item: 152        r_ui = 2.00   est = 1.94   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 2.61   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 4.68   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 2.04   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 2.65   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.97   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 1.53   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 5.00   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 1.00   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 3.18   \n"
     ]
    }
   ],
   "source": [
    "from surprise import NormalPredictor\n",
    "\n",
    "algo = NormalPredictor()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN inspired algorithms\n",
    "\n",
    "### KNN Basic\n",
    "A basic k-nearest neighbor based collaborative filtering algorithm.\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "$$\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v) \\cdot r_{vi}}\n",
    "{\\sum\\limits_{v \\in N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "or\n",
    "$$\\hat{r}_{ui} = \\frac{\n",
    "\\sum\\limits_{j \\in N^k_u(i)} \\text{sim}(i, j) \\cdot r_{uj}}\n",
    "{\\sum\\limits_{j \\in N^k_u(i)} \\text{sim}(i, j)}$$\n",
    "        \n",
    "depending on the `user_based` field of the `sim_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based KNNBasic method with cosine similarity, with stochastic gradient descent\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 1.0219\n",
      "user: 312        item: 152        r_ui = 2.00   est = 3.65   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 4.05   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.80   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.85   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 4.28   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.58   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 3.33   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.25   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.36   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.08   \n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "print('User-based KNNBasic method with cosine similarity, with stochastic gradient descent')\n",
    "bsl_options = {'method': 'sgd',\n",
    "               'n_epochs': 20,\n",
    "               }\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "algo = KNNBasic(bsl_options=bsl_options, sim_options=sim_options)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with Means\n",
    "A basic collaborative filtering algorithm, taking into account the mean\n",
    "    ratings of each user.\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v)} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_i + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - \\mu_j)} {\\sum\\limits_{j \\in\n",
    "N^k_u(i)} \\text{sim}(i, j)}$$\n",
    "\n",
    "depending on the `user_based` field of the `sim_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-based method with cosine similarity, with ALS\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9582\n",
      "user: 312        item: 152        r_ui = 2.00   est = 4.37   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.65   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.29   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.70   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 4.05   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 2.88   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.31   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.08   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.97   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.20   \n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNWithMeans\n",
    "\n",
    "print('User-based method with cosine similarity, with ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 20,\n",
    "               }\n",
    "sim_options = {'name': 'cosine', 'user_based': True}\n",
    "algo = KNNWithMeans(bsl_options=bsl_options, sim_options=sim_options)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN with ZScore\n",
    "A basic collaborative filtering algorithm, taking into account\n",
    "    the z-score normalization of each user.\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\sigma_u \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - \\mu_v) / \\sigma_v} {\\sum\\limits_{v\n",
    "\\in N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "or\n",
    "$$\\hat{r}_{ui} = \\mu_i + \\sigma_i \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - \\mu_j) / \\sigma_j} {\\sum\\limits_{j\n",
    "\\in N^k_u(i)} \\text{sim}(i, j)}$$\n",
    "\n",
    "    depending on the ``user_based`` field of the ``sim_options`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item-based method with pearson baseline similarity, with ALS\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9283\n",
      "user: 312        item: 152        r_ui = 2.00   est = 3.97   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.68   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.59   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.92   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.80   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.17   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.39   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.11   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.53   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 3.92   \n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNWithZScore\n",
    "\n",
    "print('Item-based method with pearson baseline similarity, with ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 20,\n",
    "               }\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNWithZScore(bsl_options=bsl_options, sim_options=sim_options)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Baseline and Neighborhood analysis\n",
    "A basic collaborative filtering algorithm taking into account a\n",
    "    *baseline* rating.\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{v \\in N^k_i(u)}\n",
    "\\text{sim}(u, v) \\cdot (r_{vi} - b_{vi})} {\\sum\\limits_{v \\in\n",
    "N^k_i(u)} \\text{sim}(u, v)}$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\hat{r}_{ui} = b_{ui} + \\frac{ \\sum\\limits_{j \\in N^k_u(i)}\n",
    "\\text{sim}(i, j) \\cdot (r_{uj} - b_{uj})} {\\sum\\limits_{j \\in\n",
    "N^k_u(i)} \\text{sim}(i, j)}$$\n",
    "\n",
    "    depending on the `user_based` field of the `sim_options` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "RMSE: 0.9184\n",
      "user: 312        item: 152        r_ui = 2.00   est = 3.98   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.69   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.58   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.90   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.80   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.13   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.36   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.21   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.66   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 3.95   \n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "from surprise import KNNBaseline\n",
    "from surprise import Dataset\n",
    "from surprise import get_dataset_dir\n",
    "\n",
    "def read_item_names():\n",
    "    \"\"\"Read the u.item file from MovieLens 100-k dataset and return two\n",
    "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
    "    \"\"\"\n",
    "\n",
    "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
    "    rid_to_name = {}\n",
    "    name_to_rid = {}\n",
    "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
    "        for line in f:\n",
    "            line = line.split('|')\n",
    "            rid_to_name[line[0]] = line[1]\n",
    "            name_to_rid[line[1]] = line[0]\n",
    "\n",
    "    return rid_to_name, name_to_rid\n",
    "\n",
    "# First, train the algortihm to compute the similarities between items\n",
    "#data = Dataset.load_builtin('ml-100k')\n",
    "#trainset = data.build_full_trainset()\n",
    "sim_options = {'name': 'pearson_baseline', 'user_based': False}\n",
    "algo = KNNBaseline(sim_options=sim_options)\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neighborhood analysis\n",
    "**Nearest Neighbor**: Next Let's take a look at what are the 10 nearest neighbors of the movie `Toy Story`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The 10 nearest neighbors of Toy Story are:\n",
      "Liar Liar (1997)\n",
      "Jurassic Park (1993)\n",
      "Craft, The (1996)\n",
      "Raiders of the Lost Ark (1981)\n",
      "Aladdin (1992)\n",
      "That Thing You Do! (1996)\n",
      "Mission: Impossible (1996)\n",
      "Indiana Jones and the Last Crusade (1989)\n",
      "Lion King, The (1994)\n",
      "Beauty and the Beast (1991)\n"
     ]
    }
   ],
   "source": [
    "# Read the mappings raw id <-> movie name\n",
    "rid_to_name, name_to_rid = read_item_names()\n",
    "\n",
    "# Retrieve inner id of the movie Toy Story\n",
    "toy_story_raw_id = name_to_rid['Toy Story (1995)']\n",
    "toy_story_inner_id = algo.trainset.to_inner_iid(toy_story_raw_id)\n",
    "\n",
    "# Retrieve inner ids of the nearest neighbors of Toy Story.\n",
    "toy_story_neighbors = algo.get_neighbors(toy_story_inner_id, k=10)\n",
    "\n",
    "# Convert inner ids of the neighbors into names.\n",
    "toy_story_neighbors = (algo.trainset.to_raw_iid(inner_id)\n",
    "                       for inner_id in toy_story_neighbors)\n",
    "toy_story_neighbors = (rid_to_name[rid]\n",
    "                       for rid in toy_story_neighbors)\n",
    "\n",
    "print()\n",
    "print('The 10 nearest neighbors of Toy Story are:')\n",
    "for movie in toy_story_neighbors:\n",
    "    print(movie)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SlopeOne\n",
    "\n",
    "A simple yet accurate collaborative filtering algorithm.\n",
    "\n",
    "This is a straightforward implementation of the SlopeOne algorithm\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu_u + \\frac{1}{\n",
    "|R_i(u)|}\n",
    "\\sum\\limits_{j \\in R_i(u)} \\text{dev}(i, j),$$\n",
    "\n",
    "where $R_i(u)$ is the set of relevant items, i.e. the set of items\n",
    "$j$ rated by $u$ that also have at least one common user with\n",
    "$i$. $\\text{dev}_(i, j)$ is defined as the average difference\n",
    "between the ratings of $i$ and those of $j$:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.9476\n",
      "user: 312        item: 152        r_ui = 2.00   est = 4.24   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.29   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.31   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 4.13   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.72   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.02   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.20   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.09   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.70   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.26   \n"
     ]
    }
   ],
   "source": [
    "from surprise import SlopeOne\n",
    "algo = SlopeOne()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Algorithm\n",
    "\n",
    "Typical CF data exhibit large user and item effects—systematic tendencies for\n",
    "some users to give higher ratings than others—and for some items to receive\n",
    "higher ratings than others. It is customary to adjust the data by accounting for\n",
    "these effects, which we encapsulate within the baseline estimates. \n",
    "Denote by $μ$ the overall average rating. \n",
    "\n",
    "A baseline estimate for an unknown rating $r_{ui}$ is\n",
    "denoted by $b_{ui}$ and accounts for the user and item effects:\n",
    "\n",
    "$b_{ui} = μ + b_u + b_i$ \n",
    "The parameters $b_u$ and $b_i$ indicate the observed deviations of user u and item i,\n",
    "respectively, from the average. For example, suppose that we want a baseline\n",
    "estimate for the rating of the movie Titanic by user Joe. Now, say that the\n",
    "average rating over all movies, $μ$, is 3.7 stars. Furthermore, Titanic is better\n",
    "than an average movie, so it tends to be rated 0.5 stars above the average. On\n",
    "the other hand, Joe is a critical user, who tends to rate 0.3 stars lower than the\n",
    "average. Thus, the baseline estimate for Titanic’s rating by Joe would be 3.9\n",
    "stars by calculating 3.7−0.3+0.5. In order to estimate bu and bi one can solve\n",
    "the least squares problem. \n",
    "\n",
    "More details in the theory section\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ALS\n",
      "Estimating biases using als...\n",
      "RMSE: 0.9417\n",
      "user: 312        item: 152        r_ui = 2.00   est = 4.19   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.46   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.28   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 4.00   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.83   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.03   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.36   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.17   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.74   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.13   \n"
     ]
    }
   ],
   "source": [
    "from surprise import BaselineOnly\n",
    "\n",
    "# Example using Baseline ALS\n",
    "print('Using ALS')\n",
    "bsl_options = {'method': 'als',\n",
    "               'n_epochs': 5,\n",
    "               'reg_u': 12,\n",
    "               'reg_i': 5\n",
    "               }\n",
    "algo = BaselineOnly(bsl_options=bsl_options)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD\n",
    "\n",
    "The famous *SVD* algorithm, as popularized by Simon Funk during the Netflix\n",
    "    Prize. \n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^Tp_u$$\n",
    "\n",
    "If user $u$ is unknown, then the bias $b_u$ and the factors\n",
    "$p_u$ are assumed to be zero. The same applies for item $i$\n",
    "with $b_i$ and $q_i$.\n",
    "\n",
    "To estimate all the unknown, we minimize the following regularized squared\n",
    "error:\n",
    "\n",
    "$$\\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right)$$\n",
    "\n",
    "The minimization is performed by a very straightforward stochastic gradient\n",
    "descent:\n",
    "\n",
    "$$\n",
    "b_u \\leftarrow b_u + \\gamma (e_{ui} - \\lambda b_u)\\\\\n",
    "b_i \\leftarrow b_i + \\gamma (e_{ui} - \\lambda b_i)\\\\\n",
    "p_u \\leftarrow p_u + \\gamma (e_{ui} \\cdot q_i - \\lambda p_u)\\\\\n",
    "q_i \\leftarrow q_i + \\gamma (e_{ui} \\cdot p_u - \\lambda q_i)$$\n",
    "\n",
    "where $e_{ui} = r_{ui} - \\hat{r}_{ui}$. These steps are performed\n",
    "over all the ratings of the trainset and repeated $n_epochs$ times.\n",
    "Baselines are initialized to $0$. User and item factors are randomly\n",
    "initialized according to a normal distribution, which can be tuned using\n",
    "the $init_mean$ and $init_std_dev$ parameters.\n",
    "\n",
    "You also have control over the learning rate $\\gamma$ and the\n",
    "regularization term $\\lambda$. Both can be different for each\n",
    "kind of parameter (see below). By default, learning rates are set to\n",
    "$0.005$ and regularization terms are set to $0.02$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing epoch 0\n",
      "Processing epoch 1\n",
      "Processing epoch 2\n",
      "Processing epoch 3\n",
      "Processing epoch 4\n",
      "Processing epoch 5\n",
      "Processing epoch 6\n",
      "Processing epoch 7\n",
      "Processing epoch 8\n",
      "Processing epoch 9\n",
      "Processing epoch 10\n",
      "Processing epoch 11\n",
      "Processing epoch 12\n",
      "Processing epoch 13\n",
      "Processing epoch 14\n",
      "Processing epoch 15\n",
      "Processing epoch 16\n",
      "Processing epoch 17\n",
      "Processing epoch 18\n",
      "Processing epoch 19\n",
      "RMSE: 0.9345\n",
      "user: 312        item: 152        r_ui = 2.00   est = 3.91   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.48   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.41   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.87   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.68   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.23   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.33   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.15   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.52   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.20   \n"
     ]
    }
   ],
   "source": [
    "from surprise import SVD\n",
    "\n",
    "# We'll use the famous SVD algorithm.\n",
    "algo = SVD(n_factors=100, lr_all=0.005, reg_all=0.02, verbose=True)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD++\n",
    "\n",
    "The *SVD++* algorithm, an extension of $SVD$ taking into account\n",
    "    implicit ratings.\n",
    "\n",
    "The prediction $\\hat{r}_{ui}$ is set as:\n",
    "\n",
    "\n",
    "$$\\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "|I_u|^{-\\frac{1}{2}} \\sum_{j \\\\in I_u}y_j\\right)$$\n",
    "\n",
    "Where the $y_j$ terms are a new set of item factors that capture\n",
    "implicit ratings. Here, an implicit rating describes the fact that a user\n",
    "$u$ rated an item $j$, regardless of the rating value.\n",
    "\n",
    "If user $u$ is unknown, then the bias $b_u$ and the factors\n",
    "$p_u$ are assumed to be zero. The same applies for item $i$\n",
    "with $b_i$, $q_i$ and $y_i$.\n",
    "\n",
    "Just as for $SVD$, the parameters are learned using a SGD on the\n",
    "regularized squared error objective.\n",
    "\n",
    "Baselines are initialized to $0$. User and item factors are randomly\n",
    "initialized according to a normal distribution, which can be tuned using\n",
    "the `init_mean` and `init_std_dev` parameters.\n",
    "\n",
    "You have control over the learning rate $\\gamma$ and the\n",
    "regularization term $\\lambda$. Both can be different for each\n",
    "kind of parameter (see below). By default, learning rates are set to\n",
    "$0.005$ and regularization terms are set to $0.02$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " processing epoch 0\n",
      " processing epoch 1\n",
      " processing epoch 2\n",
      " processing epoch 3\n",
      " processing epoch 4\n",
      " processing epoch 5\n",
      " processing epoch 6\n",
      " processing epoch 7\n",
      " processing epoch 8\n",
      " processing epoch 9\n",
      " processing epoch 10\n",
      " processing epoch 11\n",
      " processing epoch 12\n",
      " processing epoch 13\n",
      " processing epoch 14\n",
      " processing epoch 15\n",
      " processing epoch 16\n",
      " processing epoch 17\n",
      " processing epoch 18\n",
      " processing epoch 19\n",
      "RMSE: 0.9236\n",
      "user: 312        item: 152        r_ui = 2.00   est = 4.35   \n",
      "user: 940        item: 95         r_ui = 5.00   est = 3.62   \n",
      "user: 881        item: 161        r_ui = 3.00   est = 3.51   \n",
      "user: 436        item: 234        r_ui = 3.00   est = 3.63   \n",
      "user: 398        item: 514        r_ui = 4.00   est = 3.68   \n",
      "user: 790        item: 763        r_ui = 3.00   est = 3.40   \n",
      "user: 104        item: 290        r_ui = 4.00   est = 2.06   \n",
      "user: 32         item: 271        r_ui = 3.00   est = 3.11   \n",
      "user: 189        item: 1099       r_ui = 5.00   est = 3.52   \n",
      "user: 196        item: 302        r_ui = 4.00   est = 4.17   \n"
     ]
    }
   ],
   "source": [
    "from surprise import SVDpp\n",
    "algo = SVDpp(n_factors=20, lr_all=0.007, reg_all=0.02, verbose=True)\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)\n",
    "\n",
    "# Then show some prediction examples\n",
    "testPrediction(algo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Modified by Xiaohan Zhang, based on the original version of </i>\n",
    "\n",
    "<i>Copyright 2015, Nicolas Hug</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
